{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOcPp2J+6X0J7qsAaWS1Emx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BSPjHdXZP8wH","executionInfo":{"status":"ok","timestamp":1748073369572,"user_tz":-345,"elapsed":28537,"user":{"displayName":"Sujal Koju","userId":"05925453422640875501"}},"outputId":"ea42452a-6a69-4b00-edb9-0aa986434826"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# Install required packages\n","!pip install nltk scikit-learn scipy pandas numpy tqdm\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aOv0IlXkQFMF","executionInfo":{"status":"ok","timestamp":1748073382413,"user_tz":-345,"elapsed":4571,"user":{"displayName":"Sujal Koju","userId":"05925453422640875501"}},"outputId":"37258c6a-3e00-44af-8ecc-d452f326dbe3"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"]}]},{"cell_type":"code","source":["# Import libraries\n","import pandas as pd\n","import numpy as np\n","import re\n","import nltk\n","from tqdm import tqdm\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from scipy.sparse import csr_matrix, save_npz\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import LinearSVC\n","import joblib\n","import os\n"],"metadata":{"id":"BnQoousoQHY-","executionInfo":{"status":"ok","timestamp":1748073391282,"user_tz":-345,"elapsed":5597,"user":{"displayName":"Sujal Koju","userId":"05925453422640875501"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Download NLTK data\n","nltk.download('stopwords', quiet=True)\n","stop_words = set(stopwords.words('english'))\n","stemmer = PorterStemmer()\n"],"metadata":{"id":"DE0cQ2l6SAmx","executionInfo":{"status":"ok","timestamp":1748073396965,"user_tz":-345,"elapsed":158,"user":{"displayName":"Sujal Koju","userId":"05925453422640875501"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["\n","# 2. Set up paths\n","BASE_PATH = '/content/drive/MyDrive/SpamOrHam'\n","DATA_PATH = f'{BASE_PATH}/data/raw/spam_Emails_data.csv'\n","PROCESSED_PATH = f'{BASE_PATH}/data/processed'\n","MODEL_PATH = f'{BASE_PATH}/models'\n"],"metadata":{"id":"WVgalbK2SWFa","executionInfo":{"status":"ok","timestamp":1748073408082,"user_tz":-345,"elapsed":13,"user":{"displayName":"Sujal Koju","userId":"05925453422640875501"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["\n","# Create directories\n","for path in [PROCESSED_PATH, MODEL_PATH]:\n","    os.makedirs(path, exist_ok=True)\n"],"metadata":{"id":"IG2OqnJoSYpw","executionInfo":{"status":"ok","timestamp":1748073412551,"user_tz":-345,"elapsed":917,"user":{"displayName":"Sujal Koju","userId":"05925453422640875501"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["\n","# 3. Define preprocessing functions\n","def preprocess_data(data):\n","    \"\"\"\n","    Preprocess the dataset with only label and text columns.\n","    \"\"\"\n","    # Convert labels\n","    y = np.where(data.iloc[:, 0].str.lower() == 'spam', 1, 0)\n","\n","    # Get email text\n","    texts = data.iloc[:, 1].fillna('').values\n","    print(f\"Total emails: {len(texts)}\")\n","\n","    # Build vocabulary\n","    print(\"Building vocabulary...\")\n","    all_words = set()\n","    for text in tqdm(texts, desc=\"Building vocab\"):\n","        if pd.isna(text) or not isinstance(text, str):\n","            continue\n","        text = text.lower()\n","        text = re.sub(r'[^\\w\\s]', '', text)\n","        words = [stemmer.stem(word) for word in text.split()\n","                if word not in stop_words]\n","        all_words.update(words)\n","\n","    vocab = sorted(list(all_words))\n","    print(f\"Vocabulary size: {len(vocab)} words\")\n","\n","    # Create feature matrix\n","    print(\"\\nConverting emails to feature vectors...\")\n","    rows, cols, data_values = [], [], []\n","\n","    for i, text in tqdm(enumerate(texts), total=len(texts)):\n","        if pd.isna(text) or not isinstance(text, str):\n","            continue\n","        text = text.lower()\n","        text = re.sub(r'[^\\w\\s]', '', text)\n","        words = [stemmer.stem(word) for word in text.split()\n","                if word not in stop_words]\n","        for word in words:\n","            if word in vocab:\n","                rows.append(i)\n","                cols.append(vocab.index(word))\n","                data_values.append(1)\n","\n","    X = csr_matrix((data_values, (rows, cols)),\n","                   shape=(len(texts), len(vocab)))\n","    return X, y, vocab\n"],"metadata":{"id":"wuIpJzFWSaZk","executionInfo":{"status":"ok","timestamp":1748073415994,"user_tz":-345,"elapsed":19,"user":{"displayName":"Sujal Koju","userId":"05925453422640875501"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["\n","# 4. Load and preprocess data\n","print(\"Loading dataset...\")\n","data = pd.read_csv(DATA_PATH, encoding='latin-1', usecols=[0,1])\n","X, y, vocab = preprocess_data(data)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_p1QDp6QSdpt","outputId":"c6c2dfeb-db9a-457b-95cc-d33fef81ee5d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading dataset...\n","Total emails: 193852\n","Building vocabulary...\n"]},{"output_type":"stream","name":"stderr","text":["Building vocab:   2%|‚ñè         | 3331/193852 [00:12<19:00, 167.03it/s]"]}]},{"cell_type":"code","source":["\n","# 5. Train model\n","print(\"\\nTraining model...\")\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","model = LinearSVC(random_state=42)\n","model.fit(X_train, y_train)\n"],"metadata":{"id":"Weujl9vbSe22"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# 6. Evaluate model\n","print(\"\\nModel evaluation:\")\n","train_score = model.score(X_train, y_train)\n","test_score = model.score(X_test, y_test)\n","print(f\"Training accuracy: {train_score:.4f}\")\n","print(f\"Testing accuracy: {test_score:.4f}\")\n"],"metadata":{"id":"JCiOp1rfSgrT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# 7. Save model and vocabulary\n","print(\"\\nSaving model and vocabulary...\")\n","joblib.dump(model, f'{MODEL_PATH}/spam_classifier.pkl')\n","joblib.dump(vocab, f'{MODEL_PATH}/vocabulary.pkl')\n","\n","print(\"Training complete! Model and vocabulary saved successfully.\")"],"metadata":{"id":"LOelCZaLSiRb"},"execution_count":null,"outputs":[]}]}